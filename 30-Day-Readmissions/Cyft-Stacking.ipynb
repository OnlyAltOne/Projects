{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyft Data Investigation\n",
    "\n",
    "Goal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "\n",
    "_30-Day All-Cause Hospital Readmissions_ is a quality measure that many healthcare organizations use to track their performance. Lower readmission rates indicate better patient outcomes, while higher ones tend to indicate system problems that are negatively impacting patients. The goal of this exercise is to analyze a dataset that simulates hospitalizations for a geriatric patient population in 2015 and 2016 to predict __if a patient is likely to have a readmission based on the information available at the time of their initial admission.__\n",
    "\n",
    "You have 3 hours to complete the exercise. If you don't get through all the objectives, that's OK. After 3 hours, please finish what you're working on and send in whatever code, analyses, and visualizations (such as images) you have available. Include comments documenting any assumptions you've made as well as other ideas you would have tried if you had more time.\n",
    "\n",
    "Feel free to use the language and statistical/machine learning libraries that you are most comfortable with, and ask questions along the way if any clarifications are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dxs_summarized = {'A41': 'Sepsis',\n",
    " 'E11': 'diabetes mellitus',\n",
    " 'E56': 'Vitamin deficiency',\n",
    " 'E86': 'Dehydration',\n",
    " 'F03': 'dementia',\n",
    " 'F05': 'Delirium',\n",
    " 'F19': 'Drug abuse',\n",
    " 'G31': 'Degeneration of nervous system',\n",
    " 'G89': 'Chronic Pain',\n",
    " 'H53': 'Visual discomfort',\n",
    " 'H91': 'Hearing loss',\n",
    " 'I10': 'hypertension',\n",
    " 'I50': 'Heart Failure',\n",
    " 'I51': 'heart disease',\n",
    " 'J18': 'Pneumonia',\n",
    " 'J44': 'COPD',\n",
    " 'J45': 'asthma',\n",
    " 'M54': 'Radiculopathy/Panniculitis/Sciatica',\n",
    " 'N18': 'Chronic kidney disease',\n",
    " 'N39': 'Urinary incontinence',\n",
    " 'R05': 'cough',\n",
    " 'R26': 'abnormalities of gait and mobility',\n",
    " 'R39': 'Urgency of urination',\n",
    " 'R41': 'Cognitive functions and awareness symptoms',\n",
    " 'R51': 'headache',\n",
    " 'T88': 'Anesthesia Complication',\n",
    " 'W19': 'Fall'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_target(full):\n",
    "    full['Readmitted'] = full.groupby(['Patient'])['Patient'].transform('count')\n",
    "    full['Readmitted'] = full['Readmitted'].map({2:1,1:0})\n",
    "    full['DaysSinceAdmission'] = full[full['Readmitted'] == 1].groupby(['Patient']).diff()['AdmitDate']\n",
    "    full['<=30Days'] = (full['DaysSinceAdmission'] <= pd.Timedelta('30 days')).astype(np.int)\n",
    "    full['WillBe<=30Days'] = full[full['Readmitted'] == 1].groupby('Patient').shift(-1)[['<=30Days']].fillna(0).astype(np.int)\n",
    "    full['WillBe<=30Days'] = full['WillBe<=30Days'].fillna(0)\n",
    "    full['<=30Days'] = full['WillBe<=30Days']\n",
    "    full.drop(['Readmitted','DaysSinceAdmission','WillBe<=30Days'],axis=1,inplace=True)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_combined_dx_feats(full_orig):\n",
    "    full_orig['PrimaryDx_Dx2'] = full_orig['PrimaryDx']+full_orig['Dx2']\n",
    "    full_orig['PrimaryDx_Dx3'] = full_orig['PrimaryDx']+full_orig['Dx3']\n",
    "    full_orig['Dx2_Dx3'] = full_orig['Dx2']+full_orig['Dx3']\n",
    "    return full_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_month_feat(df):\n",
    "    df['MonthAdmit'] = df['AdmitDate'].apply(lambda x: x.month)\n",
    "    df['MonthAdmit'] = df['MonthAdmit'].map({1:'Jan',\n",
    "                                            2:'Feb',\n",
    "                                            3:'Mar',\n",
    "                                            4:'Apr',\n",
    "                                            5:'May',\n",
    "                                            6:'Jun',\n",
    "                                            7:'Jul',\n",
    "                                            8:'Aug',\n",
    "                                            9:'Sep',\n",
    "                                            10:'Oct',\n",
    "                                            11:'Nov',\n",
    "                                            12:'Dec'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nulls(full):\n",
    "    full = full.replace('@NA',np.NaN) # replace @NA\n",
    "    full = full.replace('',np.NaN) # didn't find any empty strings\n",
    "    full = full.replace(np.NaN,'')\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform data\n",
    "\n",
    "Functions created in other eda notebook and copied over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admit = pd.read_csv('../Cyft/readmissions/admissions.csv')\n",
    "claims = pd.read_csv('../Cyft/readmissions/claims.csv')\n",
    "\n",
    "full = pd.merge(admit,claims,on=['Patient','AdmitDate'])\n",
    "full['AdmitDate'] = pd.to_datetime(full['AdmitDate'])\n",
    "\n",
    "full = replace_nulls(full)\n",
    "full = add_target(full)\n",
    "#full = add_combined_dx_feats(full)\n",
    "full = add_month_feat(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_orig = full.copy()\n",
    "\n",
    "full = full.set_index('AdmitDate')\n",
    "\n",
    "for col in [col for col in full.columns if full[col].dtype == 'object' and 'Patient' not in col]:\n",
    "    dummies = pd.get_dummies(full[col],prefix=col)\n",
    "    full.drop(col,axis=1,inplace=True)\n",
    "    full = pd.concat([full,dummies],axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2856 rows and 49 columns\n",
      "test: 2938 rows and 49 columns\n"
     ]
    }
   ],
   "source": [
    "train = full['2015']\n",
    "test = full['2016']\n",
    "\n",
    "train = train.drop(['Patient'],axis=1)\n",
    "test = test.drop(['Patient'],axis=1)\n",
    "\n",
    "print('train: {} rows and {} columns'.format(*train.shape))\n",
    "print('test: {} rows and {} columns'.format(*test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy baseline is: 76.44%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy baseline is: {:.2f}%'.format(100*(1-full['<=30Days'].mean())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = train[top_feats]\n",
    "# y_train = train['<=30Days']\n",
    "# X_test = test[top_feats]\n",
    "# y_test = test['<=30Days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['<=30Days'],axis=1)\n",
    "y_train = train['<=30Days']\n",
    "X_test = test.drop(['<=30Days'],axis=1)\n",
    "y_test = test['<=30Days']\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        #params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': 5,\n",
    "    'weights': 'distance',\n",
    "    'p': 1\n",
    "}\n",
    "\n",
    "lr_params = {\n",
    "    'random_state': 0,\n",
    "    'C': 1,\n",
    "    'class_weight': 'balanced',\n",
    "    'penalty': 'l1'\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'random_state': 0,\n",
    "    'class_weight': 'balanced',\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 50,\n",
    "    'min_samples_split': 80,\n",
    "    'n_estimators': 400\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 3 Models for 1st stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "knn = SklearnHelper(clf=KNeighborsClassifier, seed=SEED, params=knn_params)\n",
    "lr = SklearnHelper(clf=LogisticRegression, seed=SEED, params=lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_oof_train, knn_oof_test = get_oof(knn, X_train, y_train, X_test)\n",
    "rf_oof_train, rf_oof_test = get_oof(rf, X_train, y_train, X_test) \n",
    "lr_oof_train, lr_oof_test = get_oof(lr, X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add out-of-fold predictions to original train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_train = train.drop(['<=30Days'],axis=1)\n",
    "oof_train['knn'] = knn_oof_train.astype(int)\n",
    "oof_train['rf'] = rf_oof_train.astype(int)\n",
    "oof_train['lr'] = lr_oof_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof_test = test.drop(['<=30Days'],axis=1)\n",
    "oof_test['knn'] = knn_oof_test.astype(int)\n",
    "oof_test['rf'] = rf_oof_test.astype(int)\n",
    "oof_test['lr'] = lr_oof_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try only preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = pd.DataFrame(knn_oof_train.astype(int))\n",
    "oof_train.columns = ['knn']\n",
    "oof_train['rf'] = rf_oof_train.astype(int)\n",
    "oof_train['lr'] = lr_oof_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test = pd.DataFrame(knn_oof_test.astype(int))\n",
    "oof_test.columns = ['knn']\n",
    "oof_test['rf'] = rf_oof_test.astype(int)\n",
    "oof_test['lr'] = lr_oof_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Top Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feats = ['Age',\n",
    " 'Dx2_E11',\n",
    " 'Dx2_E86',\n",
    " 'Dx2_F03',\n",
    " 'Dx2_I51',\n",
    " 'Dx2_T88',\n",
    " 'Dx2_W19',\n",
    " 'Dx3_',\n",
    " 'Dx3_J18',\n",
    " 'Gender_F',\n",
    " 'Gender_M',\n",
    " 'LOS',\n",
    " 'PastPCPVisits',\n",
    " 'PrimaryDx_E11',\n",
    " 'PrimaryDx_I50',\n",
    " 'PrimaryDx_J44',\n",
    " 'PrimaryDx_N18']\n",
    "\n",
    "top_feats = ['Age',\n",
    " 'Dx2_E11',\n",
    " 'Dx2_E86',\n",
    " 'Dx2_F03',\n",
    " 'Dx2_F05',\n",
    " 'Dx2_F19',\n",
    " 'Dx2_G31',\n",
    " 'Dx2_I51',\n",
    " 'Dx2_R39',\n",
    " 'Dx2_T88',\n",
    " 'Dx2_W19',\n",
    " 'Dx3_',\n",
    " 'Dx3_H53',\n",
    " 'Dx3_J18',\n",
    " 'Dx3_M54',\n",
    " 'Dx3_R05',\n",
    " 'Dx3_R26',\n",
    " 'Gender_F',\n",
    " 'Gender_M',\n",
    " 'LOS',\n",
    " 'PastPCPVisits',\n",
    " 'PrimaryDx_A41',\n",
    " 'PrimaryDx_E11',\n",
    " 'PrimaryDx_I50',\n",
    " 'PrimaryDx_J44',\n",
    " 'PrimaryDx_N18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train = train[top_feats].copy()\n",
    "oof_train['knn'] = knn_oof_train.astype(int)\n",
    "oof_train['rf'] = rf_oof_train.astype(int)\n",
    "oof_train['lr'] = lr_oof_train.astype(int)\n",
    "#oof_train['lr'] = oof_train['lr'].map({0:0,1:5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test = test[top_feats].copy()\n",
    "oof_test['knn'] = knn_oof_test.astype(int)\n",
    "oof_test['rf'] = rf_oof_test.astype(int)\n",
    "oof_test['lr'] = lr_oof_test.astype(int)\n",
    "#oof_test['lr'] = oof_test['lr'].map({0:0,1:5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 2nd stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'criterion': ['gini'], 'n_estimators': [1000], 'max_depth': [3, 4, 5, 6, 7], 'max_features': ['auto'], 'class_weight': ['balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "params['criterion'] = ['gini']#,'entropy']\n",
    "params['n_estimators'] = [1000]#[400,600,800]#[200]\n",
    "params['max_depth'] = [3,4,5,6,7]#,10,15]#[20,25,30,35,40,45,50,55,60]#[15,20,25]\n",
    "params['max_features'] = ['auto'] #['auto','sqrt']\n",
    "#params['min_samples_split'] =  [40,60,80,100] #[2,4,6,8,10]\n",
    "#params['min_samples_leaf'] = [1,2,3]\n",
    "params['class_weight'] = ['balanced']\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "stack_grid = GridSearchCV(rf,params,scoring='f1')\n",
    "stack_grid.fit(oof_train,y_train)\n",
    "\n",
    "stack = stack_grid.best_estimator_\n",
    "stack.fit(oof_train,y_train)\n",
    "stack_pred = stack.predict(oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbno/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [1000], 'base_estimator__loss': ['log'], 'algorithm': ['SAMME']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "              \"n_estimators\": [1000],\n",
    "              \"base_estimator__loss\": ['log'],\n",
    "              \"algorithm\": ['SAMME'],\n",
    "             }\n",
    "\n",
    "\n",
    "LR = LogisticRegression()\n",
    "SGD = SGDClassifier()\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = SGD)\n",
    "\n",
    "# run grid search\n",
    "grid_search_ABC = GridSearchCV(ABC, param_grid=param_grid,scoring = 'accuracy')\n",
    "grid_search_ABC.fit(oof_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(base_estimator=LogisticRegression(),n_estimators=1000,learning_rate=1)\n",
    "ab.fit(oof_train,y_train)\n",
    "stack_pred = ab.predict(oof_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the predictions\n",
    "\n",
    "This is an interesting outcome. Talk through what the precision vs recall mean and calculate them out if there were real numbers involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(y_test)\n",
    "preds['knn'] = knn_oof_test.astype(int)\n",
    "preds['lr'] = lr_oof_test.astype(int)\n",
    "preds['rf'] = rf_oof_test.astype(int)\n",
    "preds['stack'] = stack_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: knn\n",
      "\tprecision: 0.433\n",
      "\trecall: 0.133\n",
      "\taccuracy: 0.759\n",
      "Model: lr\n",
      "\tprecision: 0.495\n",
      "\trecall: 0.726\n",
      "\taccuracy: 0.766\n",
      "Model: rf\n",
      "\tprecision: 0.504\n",
      "\trecall: 0.595\n",
      "\taccuracy: 0.771\n",
      "Model: stack\n",
      "\tprecision: 0.591\n",
      "\trecall: 0.367\n",
      "\taccuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "for col in [col for col in preds if '<=30Days' not in col]:\n",
    "    y_pred = preds[col]\n",
    "    print('Model: {}'.format(col))\n",
    "    print('\\tprecision: {:.3f}\\n\\trecall: {:.3f}\\n\\taccuracy: {:.3f}'.format(precision_score(y_test,y_pred),\n",
    "                                                       recall_score(y_test,y_pred),\n",
    "                                                       accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=30Days    1.000000\n",
       "knn         0.132548\n",
       "lr          0.726068\n",
       "rf          0.594993\n",
       "stack       0.366716\n",
       "dtype: float64"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[preds['<=30Days'] == 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
