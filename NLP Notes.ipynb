{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "### Stanford 2012 video set - Classic NLP topic intro\n",
    "playlist: https://www.youtube.com/watch?v=qCA1Dk_Ih_c&list=PLqNqLI7n_fDbisqKkkAzrFpWQOg8E6KEf&index=37#t=4.500449\n",
    "\n",
    "no course material available\n",
    "\n",
    "### Stanford 2017 video set - Deep Learning for NLP\n",
    "playlist: https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=1&t=115s\n",
    "\n",
    "course material: http://cs224d.stanford.edu/syllabus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level NLP\n",
    "- Input\n",
    "- Morphological analysis\n",
    "    - parts of words\n",
    "- Syntactic Analysis\n",
    "    - structure of sentance nouns, verbs\n",
    "- Semantic interpretation\n",
    "    - meaning of sentances\n",
    "- Discourse processing\n",
    "    - sentance meaning based on context\n",
    "    \n",
    "- Human language\n",
    "    - unique over other data because it always has a goal\n",
    "    - issue of 100ks of vocabulary = problem of sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning/Representation Learning\n",
    "Very different from standard ML\n",
    "    - ML needs huge manual work of hand designed features\n",
    "        - Was doing numeric optimization, but humans were doing all the feature engineering\n",
    "    - DL automatically learns features/representations\n",
    "        - Different versions of NN\n",
    "        - Stacked logistic regressions, but vastly different\n",
    "        - Learned features are easy to adapt and update\n",
    "Why?\n",
    "    - Compute power + huge online free data + creative algos and advances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word meaning\n",
    "Vector representation of meaning of words\n",
    "    - High dimensional vector space\n",
    "    - Minimally 25 dimensional\n",
    "    - Commonly 300\n",
    "    - Crazy specific would be 1000 dimensional vectors\n",
    "    \n",
    "Act as semantic spaces\n",
    "    - words cluster together\n",
    "    - directions act as components of meaning\n",
    "    - these axis don't necessarily don't mean anything, though they can be interpreted sometimes\n",
    "    - PCA or Tsne for picturing the data, but can be misleading\n",
    "\n",
    "## Methods\n",
    "Dependency Parsing\n",
    "Sentiment Analysis\n",
    "Dialogue Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "Fragment\n",
    "    - main- mainly\n",
    "Filled pause\n",
    "    - uh, um, etc.\n",
    "Lemma\n",
    "    - cat, cats, same stem\n",
    "    - they, their\n",
    "Wordforms\n",
    "    - cat cats\n",
    "Corpus\n",
    "    - data set of words\n",
    "    - google n-grams \n",
    "        - Tokens ~1 trillion\n",
    "        - Types ~ 13 million\n",
    "Word shape\n",
    "    - capital vs lowercase vs other use\n",
    "OOV out of vocabulary words\n",
    "    - words that have never been seen before\n",
    "Open vs closed\n",
    "    - Clsoed - You know the words being used in entire corpus\n",
    "VSMs\n",
    "    - Vector Space Models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary\n",
    "Types\n",
    "    - unique words\n",
    "Tokens\n",
    "    - how many instances of that type in a peice of text\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "    - keep or drop appostraphes?\n",
    "        - finland's capital = finland or findlands or finland's\n",
    "    - keep dashes?\n",
    "        - Hewlet-Packard = Hewlet Packard\n",
    "        - state-of-the-art = state of the art\n",
    "    - other languages\n",
    "        - german compound splitting\n",
    "        - chinese has no spaces so you need to break them out\n",
    "        - japapanese 4 alphabets used intermingly\n",
    "        - greedy algo works well when splitting chinese sentances into distinct words, doesn't work well with english because it has a lot of long words\n",
    "\n",
    "#### Normalization\n",
    "Want all of the same word form to match, define equivelence matching\n",
    "\n",
    "Equivelence\n",
    "    - USA = U.S.A. = United States\n",
    "Asymmetric expansion\n",
    "    - window = window = windows\n",
    "    - windows = window = Windows = window\n",
    "    - Windows = Windows\n",
    "In general use symmetric expansion since it is simpler, occassionally reducing all letters to lower case, with possible exceptions of proper nouns\n",
    "\n",
    "Case Folding\n",
    "    - Fed vs fed\n",
    "    - General Motors\n",
    "\n",
    "#### Morphology\n",
    "Morphemes - smallest meaningful units that make up words\n",
    "\n",
    "Stem\n",
    "    - core unit\n",
    "Affixes\n",
    "    - bits adhere to stem\n",
    "    - gramatical functions\n",
    "        \n",
    "#### Morphology - Stemming\n",
    "Simplified version of lemmatization\n",
    "Porter's algo\n",
    "    - most common english stemmer\n",
    "    - list of rules that removes suffixes\n",
    "    \n",
    "Example: ing\n",
    "    - if word has a vowel before ing, remove, else don't\n",
    "        king = king\n",
    "        anything = anyth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentance Segmentation\n",
    "End of sentance is more tricky than you think. Basically just define a set a features that could incdicate eos and then use them in any ML algo to determine if it is\n",
    "\n",
    "Period\n",
    "    - 4.0\n",
    "    - Bye.\n",
    "    - Dr. Who\n",
    "Others are easy\n",
    "    - !,?\n",
    "\n",
    "Binary classifier to determine end of sentance\n",
    "    - lots of blank lines after? eos\n",
    "    - final punctuation ?, !? eos\n",
    "    - period? are you after an abbreviation? not eos, else eos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Minimum edit dinstance\n",
    "How similar are two strings? or two sentances. Minimize path cost from initial state to goal state\n",
    "\n",
    "---\n",
    "\n",
    "Minimum number of editing oporations\n",
    "    - insertion, deletion, substitution\n",
    "    - to transform one into another\n",
    "    \n",
    "Example\n",
    "    - inte*ntion\n",
    "    -  execution\n",
    "    - dss is    \n",
    "    - 1220120000 = 8\n",
    "\n",
    "#### Levenschtein distance\n",
    "    - insertion = 1\n",
    "    - deletion = 1\n",
    "    - substitution = 2\n",
    "    \n",
    "#### Dynamic programming for Backtrace and Alignment\n",
    "    - figuring out which operation should be used at each char\n",
    "\n",
    "#### Weighted\n",
    "Spelling\n",
    "    - e often confused with a or o\n",
    "    - distance on the keyboard also possibility\n",
    "Biology\n",
    "    - some errors more common from others\n",
    "Add a lookup table for cost function for\n",
    "    - insertion, deletion, substitution\n",
    "\n",
    "#### Use Cases\n",
    "Spell correction\n",
    "    - distances and weights\n",
    "    - graffe = graf or graft or grail or giraffe\n",
    "Computational biology\n",
    "    - similarity and scores\n",
    "Named entity extraction\n",
    "    - IBM Inc announced today..\n",
    "    - IBM profits\n",
    "    - Stanford President John Hennessy announced yesterday..\n",
    "    - for Stanford University President John Hennessy..\n",
    "    \n",
    "#### Computational Biology - Advanced Minimum Edit Distance\n",
    "Maximize similarity\n",
    "\n",
    "Needleman-Wunsch\n",
    "    - negative cost for deletion and insertion\n",
    "    - positive value for substitution\n",
    "    - overlapping unmatched at beginning and end unpenalized because it could be a snipit with excess\n",
    "    - find substrings with maximum alignment anyhere within the space\n",
    "Smith-Waterman\n",
    "    - add additional option to start over from zero if score gets to high\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "Assign a probability to a sentance\n",
    "Machine translation\n",
    "    - P(high winds tonight) > P(large winds tonight)\n",
    "Spelling correction\n",
    "    - P(about fifteen minutes from) > P(about fifteen minuets from)\n",
    "Speech recognition\n",
    "    - P(I saw a van) > P(eyes awe of an)\n",
    "Summarization, Q&A, everywhere!\n",
    "\n",
    "Any type of model that computes probability of\n",
    "    - Joint prob P(w1,w2,w3)\n",
    "    - Conditional prob P(w1|w2,w3)\n",
    "\n",
    "Chain rule of prob\n",
    "    - P(its water is transparent) = P(its) * P(water|its) * P(is|its water) * P(transparent|its water is)\n",
    "\n",
    "Too many computations so use Markov assumption, so use last or two last words insead\n",
    "    - P(transparent| water is) ~= P(transparent|its water is)\n",
    "    \n",
    "#### N-gram models\n",
    "Multiple previous words, but in general it is insufficient because language has long-distance dependencies\n",
    "    - 'The __computer__ that I just put into the machine room on the fifth floor __crashed__'\n",
    "    - In practice, we can get away with n-grams (of 3,4,5) which will account for the local information and most of the meaning\n",
    "\n",
    "N-gram probablility (basically tf-idf except with n-1 insead of document)\n",
    "    - how often does the word want follow the word i in the corpus?\n",
    "    - get a matrix of all words and all n-1 words\n",
    "    - divide by the count of each n-1 count within the whole corpus\n",
    "\n",
    "Using that we can then take the sentance probabilities by calculating the approximate probability as:\n",
    "    - P(<s> I want english food </s>) = P(I|</s>) * P(want|I) * P(english|want) * P(</s>|food) = .000031\n",
    "\n",
    "What do these probs say?\n",
    "    - P(english|want) = .0011\n",
    "    - P(chinese|want) = .0065\n",
    "    - P(to|want) = .6\n",
    "    \n",
    "Facts\n",
    "    - World fact: because corpus is of food, chinese is more popular than english food\n",
    "    - Grammatic fact: want to, inflection\n",
    "\n",
    "Practically its normal to add the log probs as opposed to multiply, since computationally it is less expensive\n",
    "    - p1 * p2 = log(p1) + log(p2)\n",
    "    \n",
    "#### Evaluation\n",
    "Extrinsic\n",
    "    - Standard ML approach, try two models and eval\n",
    "Intrinsic (Perplexity)\n",
    "    - Only works when test data looks just like training data\n",
    "    - is the weighted equivelent branching factor\n",
    "    - Guessing numerical digits, perplexity = 10\n",
    "    - Guessing 30k names at Microsoft, perplexity = 30k\n",
    "    - 1/4 * 1/4 * 1/4 * 1/30k = 53\n",
    "    - Then you can see which model bigram vs trigram perplexities\n",
    "    \n",
    "If you test on WSJ but test on Shakespear it won't work\n",
    "    \n",
    "Need to find a way to deal with bigrams that exist in the test set that don't exist in the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Smoothing\n",
    "Goal of smoothing is to use the count of things we've seen to help estimate things we've never seen\n",
    "\n",
    "---\n",
    "\n",
    "Add-one\n",
    "    - good for text classification, not language modeling\n",
    "Extended interpolated Kneser-Ney\n",
    "    - most commonly used\n",
    "Stupid Backoff\n",
    "    - works very well on very large datasets (google ngrams)\n",
    "\n",
    "#### Add-one estimation (used more in text classification)\n",
    "Change all counts from zero to one and then do the same thing as above\n",
    "Laplace-counts\n",
    "    - Add ones and evaluate with rest of original data\n",
    "Reconsistituted counts\n",
    "    - Change original counts by subracting ones added to rest of words\n",
    "\n",
    "#### Backoff and Interpolation\n",
    "Sometimes it helps to use less context\n",
    "\n",
    "Backoff\n",
    "    - use a trigram if you have good evidence of it, otherwise backoff to bigram if good evidence, or unigram if best\n",
    "Interpolation\n",
    "    - use a mixture of unigram, bigram, and trigram all the time\n",
    "    - just try to best match the highest prob combo of all\n",
    "    - better than backoff\n",
    "    \n",
    "Simple linear interpolation\n",
    "    - weight each uni, bi, tri with its own lambda\n",
    "Conditional interpolation\n",
    "    - weight each uni, bi, tri with a weighted lambda based on the previous 1 or 2 words\n",
    "    - Train lambdas in a heldout dataset\n",
    "\n",
    "#### OOV Out of Vocabulary\n",
    "Take words that are very rare words and change them to UNK. Then train bi, tris on word, unk, word\n",
    "\n",
    "\n",
    "#### Efficiencies\n",
    "Count Pruning\n",
    "    - Only store ngrams with count > threshold\n",
    "    - remove all singletons\n",
    "Entropy Pruning\n",
    "    - Remove ngrams that don't contribute to reduced perplexity\n",
    "Data structures\n",
    "    - tries\n",
    "Bloom filters\n",
    "    - approximate LMs\n",
    "Storage\n",
    "    - Store words as indexes, not strings\n",
    "    - Huffman coding\n",
    "Quantize\n",
    "    - smaller bytes than full floats for probs\n",
    "\n",
    "#### Advanced Language Modeling\n",
    "Discriminative models\n",
    "    - choose ngram weights that best fit the task to imporove a task\n",
    "Parsing\n",
    "Cached\n",
    "\n",
    "Good-Turing Smoothing\n",
    "    - 10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish\n",
    "    - N10 = 1, N3 = 1, N2 = 1, N1 = 3\n",
    "    - use estimate of N1 for unseen things\n",
    "    - reserve probability mass for unseen things which is now 3/18\n",
    "    - basically all the other counts are discounted a bit, so instead of 10 carp its now more like 9.2 carp\n",
    "    \n",
    "Absolute Discounting\n",
    "    - save time, just subtract .75 from all counts\n",
    "    \n",
    "Kneser-Ney Smoothing\n",
    "    - Goal to better estimate for probilities of lower-order unigrams\n",
    "    - for each word, finds the number of ngrams it completes divided by the whole set of words\n",
    "    - Often used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction\n",
    "Non-word errors\n",
    "    - mispelling is an unreal word\n",
    "Real-word errors\n",
    "    - typographical errors\n",
    "        - three => there\n",
    "    - cognitive errors\n",
    "        - peice => peace\n",
    "        - too => two\n",
    "\n",
    "Non-word\n",
    "    - big dictionary, if its not in it its spelled wrong\n",
    "    - generate candidates, choose best by \n",
    "        - shortest weighted edit distanc\n",
    "        - highest noisy channel prob\n",
    "\n",
    "#### Noisy Channel\n",
    "error model or edit prob\n",
    "    - Determine probability of each type of error for each word based on dictionary of mispellings\n",
    "\n",
    "#### Spell correction edit distance\n",
    "Damerau-Levenshtein\n",
    "    - insertion, deletion, substitution, transposition of adjacents\n",
    "\n",
    "Spelling errors\n",
    "    - 80% within 1 edit distance\n",
    "    - almost all in 2 edit distance\n",
    "    - also includes spaces or ommited spaces\n",
    "\n",
    "#### Non-word errors\n",
    "Take the channel model and multiple by the language model\n",
    "    - P(channel) * P(language model)\n",
    "    - P(error) * P(word) to get the true error of word\n",
    "\n",
    "uni vs bi model\n",
    "    - uni says actress = 2.6, across = 2.7\n",
    "    - bi says P(actress|versatile) = .000021, P(whose|actress) = .0010, P(across|versatile) = .000021, P(whose|across) = .000006\n",
    "    - multiply them and take largest\n",
    "    \n",
    "#### Real-word errors\n",
    "Assumes only a single spelling error per sentance, only change is in the channel model including the probability of the word being no error\n",
    "\n",
    "#### Phoenetic model\n",
    "    - model mispellings by pronunciations\n",
    "\n",
    "#### Channel Model improvements\n",
    "    - richer edits\n",
    "        - le => al\n",
    "        - ph => f\n",
    "        - ent => ant\n",
    "#### Add classifier\n",
    "    - whether vs weather +-10 words including cloudy or sunny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Niave Bayes was originally created to figure out the federalist papers authors\n",
    "    - Authorship attribution\n",
    "    - Gender identification\n",
    "        - f more pronouns, m more facts\n",
    "    - Sentiment analysis\n",
    "    - Auto subject\n",
    "\n",
    "Naive Bayes assumptions\n",
    "    - Bag of Words, position doesn't matter\n",
    "    - Conditional Independence, feature probs independent of class\n",
    "    - underflow prevention log(x) + log(y) \n",
    "    - very good for large data\n",
    "\n",
    "Tweak Performance\n",
    "    - Domain specific features and weights\n",
    "    - collapse terms like chemical formulas and part numbers\n",
    "    - upweighting \n",
    "        - counting title words twice\n",
    "        - counting first senatance words twice\n",
    "        - counting words in a sentance that contains title words twice\n",
    "\n",
    "#### Multiclassification grouping\n",
    "Macroaveraging\n",
    "    - average all individual confusion matrixes precisions together\n",
    "Microaveraging\n",
    "    - add all confusion tables together then take precision on that\n",
    "\n",
    "Precision/Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Use to analyze the sentiment of different aspects of the product\n",
    "    - ease of use is 4.5\n",
    "    - customer support is 3.0\n",
    "    \n",
    "Twitter sentiment analysis overtime\n",
    "    - specific brands\n",
    "\n",
    "#### Topology of Sentiment\n",
    "    - emotion\n",
    "        - brief, evaluation of a major event\n",
    "    - mood \n",
    "        - diffuse non-caused, long-duration\n",
    "    - Interpersonal stance\n",
    "        - friendly, flirty, distant\n",
    "    - Attitude (sentiment)\n",
    "        - affectively colored beliefs, liking loving\n",
    "    - Personality traits\n",
    "        - typical stable behavior, nervous, anxious\n",
    "        \n",
    "#### Negation\n",
    "Take every word between a negation and the following punctuation and add a NOT_ to it\n",
    "    - didn't like this movie, but I\n",
    "    - didn't NOT_like NOT_this NOT_movie, but I\n",
    "    \n",
    "#### Boolean Niave Bayes\n",
    "Idea that the counts of words dont actually matter as much as the word being there at all. \n",
    "    - clip all counts at 0 or 1\n",
    "    - good for sentiment analysis tasks\n",
    "    \n",
    "#### MaxEnt and SVMs oftentimes work best\n",
    "\n",
    "#### Datasources for word annotations\n",
    "    - LIWC\n",
    "    - General Inquirer\n",
    "\n",
    "#### Building our own lexicon for single word sentiment analysis (learn domain specific informaiton)\n",
    "Semisupervised method to label word polarity\n",
    "    - anything separated by and have same polarity\n",
    "    - anything separated by but have opposite polarity\n",
    "    \n",
    "Label by hand ~600 and search 'was nice and'\n",
    "    - find nice and helpful, or nice and classy\n",
    "    \n",
    "Create a classifier with the count(and) and the count(but) and then cluster which words are most similar\n",
    "\n",
    "Turney Bootstrap a phrases lexicon\n",
    "    - use pos labels to extract certain patterns of words adj noun noun\n",
    "    \n",
    "Pointwise mutual information\n",
    "    - how often do two words occur together versus by themselves\n",
    "    \n",
    "F-score should be usedf or severe class imbalances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative vs Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction IE\n",
    "Goal to produce a structured representation of relevant information, DB or knolwedgebase\n",
    "\n",
    "They extract\n",
    "    - who, what, where, when\n",
    "    - factuals\n",
    "    - \n",
    "\n",
    "#### Named Entity Recognition\n",
    "    - Find name and the limits of the name. Then tag with Person, Date, Location, Organization.\n",
    "    - Question answering, lots of answers are named entities\n",
    "    - Relationships between NEs\n",
    "    \n",
    "How to evaluate\n",
    "    - per enity not per token\n",
    "    - can have problems that are more delicate than precision and recall can tell\n",
    "    - the left boundary can be incorrect while the right can be correct\n",
    "    - F1 is used though the score can be misleading when classifying named entities incorrectly as opposed to not trying at all\n",
    "    \n",
    "#### Sequence labeling: IO (input output) vs IOB encoding\n",
    "    - hand labeled training data\n",
    "    - PER coding, B-PER and I-PER coding\n",
    "    - IOB slows down very much and doesn't offer much added benefit. In addition, in practice it usually doesn't characterize the seperate names well.\n",
    "    - Sequence model, multiple per\n",
    "Features\n",
    "    - substring features like ends in field (location) or has oxa in it (drug name) \n",
    "    - word shapes Xx-xxx\n",
    "\n",
    "#### Sequence (Chunking) problems\n",
    "Used in all sorts of cases\n",
    "    - POS tagging\n",
    "    - Word segmentation for other languages characters\n",
    "    - Entity recog\n",
    "    - Text QA segmentation, where does question end, answer begin?\n",
    "\n",
    "\n",
    "#### MEMM/CMM Maximum Entropy/Conditional Markov Models\n",
    "    - Beam Inference\n",
    "        - takes top 5 ideas per sequence\n",
    "    - Viterbi Inference\n",
    "        - needs small windows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation Extraction\n",
    "Useful in queston/answering and building knowledgebases from text\n",
    "\n",
    "ACE - Automated Content Extraction Relations\n",
    "    - physical-location PER-GPE\n",
    "        - he was in tennessee\n",
    "    - part-whole-subsidiary ORG-ORG\n",
    "        - XYZ, the parent company of ABC\n",
    "        \n",
    "UMLS - Unified Medical Language System\n",
    "    \n",
    "RDF - Resourse Description Framework triples\n",
    "\n",
    "Ontological relations\n",
    "    - IS-A\n",
    "    - instance-of\n",
    "    \n",
    "#### Manual intuition of IS-A\n",
    "Rules like\n",
    "    - y, such as x\n",
    "    - x or other y\n",
    "    - person, position of org\n",
    "        - George Marshall, Secretary of State of the United States\n",
    "\n",
    "What types of relations hold between different entities\n",
    "    - Drug (cures|causes) disease\n",
    "    \n",
    "#### Supervised Classification of Relations (powerful when test data will be in same domain as train data)\n",
    "Steps\n",
    "    - choose relations to extract\n",
    "    - choose relevant named entities\n",
    "    - find and label data\n",
    "        - choose corpus\n",
    "        - label named entities (automated i suppose)\n",
    "        - hand-label the relations between these entities\n",
    "        - break into training, dev, test\n",
    "\n",
    "Break into two problems\n",
    "    - find all pairs of entities\n",
    "    - train a classifier to determine if two named entities are related, give a yes no\n",
    "    \n",
    "#### Types of features for supervised\n",
    "- Trigger list ex kinship\n",
    "    - parent, mother, father, aunt\n",
    "- Gazeteer list (list of proper nouns)\n",
    "- Headwords\n",
    "- Words/bigrams immediately left or right of the entities\n",
    "- Bag of words of the entities\n",
    "- Bag of words between the mentions\n",
    "- Entity types\n",
    "    - PER, GPE, GEO\n",
    "- Syntactic chunk sequence\n",
    "    - NP NP PP VP NP NP\n",
    "    \n",
    "#### Semi/Unsupervised Relation Extraction\n",
    "Bootstrapping\n",
    "    - Start with 3 seed examples:\n",
    "        - Isaac Asimov - The Robots of Dawn\n",
    "        - JR Tolkien - Fellowship\n",
    "        - Shakespeare - The Comedy of Errors\n",
    "    - Find examples of sentances with both:\n",
    "        - The Fellowship, by JR,\n",
    "        - Th Fellowship, one of JR's most\n",
    "    - Extract patterns\n",
    "        - ?x, by ?y,\n",
    "        - ?x, one of y's\n",
    "Distantly supervised\n",
    "    - \n",
    "Textrunner\n",
    "    - \n",
    "    * most interesting*\n",
    "\n",
    "No way to score these, so often times you spotcheck a subset of predictions on a class manually to produce an approximate prcision score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "#### Classic 8:\n",
    "    - noun, verb, adjective, adverb, preposition, conjunction, pronoun, interjection\n",
    "\n",
    "These are really broken don further\n",
    "    - Nouns - Proper vs normal\n",
    "    \n",
    "Open and closed classes\n",
    "    - Closed: determiners, pronoun - which have a fixed set of word that aren't being invented\n",
    "    - Open: verbs, nouns, etc - commonly invented\n",
    "\n",
    "Idea of POS tagging\n",
    "    - to characterize the word in the context of running text\n",
    "    - Ex: back\n",
    "        - the back door - jj (adjective)\n",
    "        - on my back - nn (noun)\n",
    "        - win the voters back - rb (adverb)\n",
    "\n",
    "Uses\n",
    "    - how to pronounce 'lead'\n",
    "        - if its a verb its 'leed' if its a noun its 'led'\n",
    "    - extract base-noun phrases\n",
    "\n",
    "#### Features for POS\n",
    "- word\n",
    "- lowercase % chance (capitalized words at beginning of sentances are usually predicted as nnp)\n",
    "- prefix/suffix important(ly) = rb\n",
    "- capitalization = nnp usually\n",
    "- word-shapes 35-year = jj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Structure\n",
    "Constituency - phrase structure\n",
    "    - can move them around\n",
    "        - john talked [about drugs] [to the children]\n",
    "        - john talked [to the children] [about drugs]\n",
    "    - substitute/expand them\n",
    "        - i sat [on the box/right on top of the box/over there]\n",
    "\n",
    "VP = VB*\n",
    "NP = NN*\n",
    "\n",
    "Dependency - which words depend on which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Algorithms\n",
    "\n",
    "Based on sources:\n",
    "- https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278\n",
    "\n",
    "Benchmark Datasets:\n",
    "- https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Methods:\n",
    "- TF-IDF\n",
    "- Linear SVM\n",
    "- Word embeddings (word2vec)\n",
    "- Attention based NN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN\n",
    "\n",
    "All typically employ a meta architecture of:\n",
    "- Embedding\n",
    "- Deep representation\n",
    "- Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Haven't done\n",
    "https://www.youtube.com/watch?v=FhReDSvZ35s\n",
    "\n",
    "12-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word senses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vector Space Representations\n",
    "#### Term-Document is a specific form of Word-Context (attributional similarity)\n",
    "    - columns are documents\n",
    "    - rows are words\n",
    "    - values are counts\n",
    "#### Pair-Pattern (relational similarity)\n",
    "    - rows are pairs of words\n",
    "    - columns are patterns in which those words are found\n",
    "    - values are \n",
    "    - example: carpenter:wood, X cuts Y, X works with Y\n",
    "Latenet relation hypothesis:\n",
    "    - pair-pattern words can find that carpenter:wood, glassblower:glass, potter:clay all have similar semantic relations\n",
    "    \n",
    "#### Attributional vs Relational Similarity\n",
    "dog and wolf have a relatively high degree of attributional similarity, whereas dog : bark and cat : meow have a relatively high degree of relational similarity\n",
    "\n",
    "#### Semantic/Taxonomic Similarity\n",
    "Words that share a hynonym\n",
    "\n",
    "#### Semantically Associated\n",
    "Words are semantically associated if they tend to co-occur frequently (e.g., bee and honey) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may be taxonomically similar and semantically associated (doctor and nurse), taxonomically similar but not semantically associated (horse and platypus), semantically associated but not taxonomically similar (cradle and baby), or neither semantically associated nor taxonomically similar (calculus and candy).\n",
    "    \n",
    "source: http://www.jair.org/media/2934/live-2934-4846-jair.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Beauty of Word Embeddings\n",
    "\n",
    "It’s important to appreciate that all of these properties of WW are side effects. We didn’t try to have similar words be close together. We didn’t try to have analogies encoded with difference vectors. All we tried to do was perform a simple task, like predicting whether a sentence was valid. These properties more or less popped out of the optimization process.\n",
    "\n",
    "This seems to be a great strength of neural networks: they learn better ways to represent data, automatically. Representing data well, in turn, seems to be essential to success at many machine learning problems. Word embeddings are just a particularly striking example of learning a representation.\n",
    "\n",
    "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector representations\n",
    "Meaning\n",
    "    - Signifier\n",
    "    - Ontological information - WordNet NLTK\n",
    "\n",
    "One-hot vector notation\n",
    "    - symbolic representation\n",
    "    - Doesn't have any inherent notion of similarity between words motel and hotel. The dot product would equal 0\n",
    "    - [0,0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
    "    - [0,0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "Distributional similarity\n",
    "    - Look at which words are around other words\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings: Word2vec, GloVe\n",
    "Skipgram style is to not to do a full update but only the window words update\n",
    "\n",
    "#### Word2vec\n",
    "Create vectors of words that represent the probability of word a (within window w) given word b\n",
    "\n",
    "example with window 2\n",
    "    - a monkey ate an apple and threw the core away\n",
    "    - = ====== .1  .2   b   .01   .3  === ==== ====\n",
    "\n",
    "In words\n",
    "    - Radius m\n",
    "    - For every word t in text\n",
    "    - Probablility distribution for every word given every context word\n",
    "\n",
    "2 data sets\n",
    "    - Word vocabulary (onehot vector)\n",
    "    - Vector representation of all probability distributions for every word given other words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec intuition\n",
    "So word2vec at its core is a 1 layer NN where the word embeddings that are learned are actually the 'input vector' aka the w1 weights I believe. \n",
    "\n",
    "There are two ways to organize the tuning efficiency which arise from the fact that updating all the vectors for a huge corpus would be costly. \n",
    "\n",
    "## Training Approaches\n",
    "##### Hierarchical Softmax and Negative sampling\n",
    "Instead of the output vector representing the probability of a word, it can be a binary tree or could randomly sample bad samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to solidify these two below - currently not concrete ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram\n",
    "Instead of doing this for each word and then updating, you can run the window through the entire corpus, get the counts for each word every time it appears, and then make an update\n",
    "\n",
    "Then do SVD to reduce the counts matrix down to a certain level of dimensions (not good to roll new words into it, also bad for very large datasets)\n",
    "\n",
    "Hacks\n",
    "    - Weight values of words further away from center with less count\n",
    "    - cap counts at 100\n",
    "    - ignore high freq words\n",
    "    \n",
    "#### GloVe\n",
    "Count entire corpus for the overall statistics of how often the words coappear\n",
    "\n",
    "Initializing words randomly vs with word2vec or glove \n",
    "    - good for senantic similarity but not for sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy\n",
    "#### Max-Margin Loss (similar concept to svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forwardprop\n",
    "Evaluae the function at a given value\n",
    "\n",
    "#### Backprop\n",
    "Train the network and update it based on forward prop value\n",
    "\n",
    "    - Take the partial differential aka local gradient at each node during forward prop\n",
    "    - then find total gradient\n",
    "    - back propigate it by multiplying it against the originally calculated local gradients at each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric to choose\n",
    "Summarization\n",
    "    - Rouge\n",
    "Translation\n",
    "    - Blue\n",
    "Unbalanced Classification\n",
    "    - F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency grammars\n",
    "Describe structure of language - Historically used Context free grammar\n",
    "\n",
    "Treebanks\n",
    "    - reuseable\n",
    "    \n",
    "Dependencies - single head and form a tree\n",
    "    - head\n",
    "        - dependents\n",
    "        - head\n",
    "\n",
    "I'll give a talk tomorrow on bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
