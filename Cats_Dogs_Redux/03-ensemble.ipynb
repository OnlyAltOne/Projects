{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs Cat Redux\n",
    "\n",
    "---\n",
    "\n",
    "Notebook to prototype different architectures rapidly. Using sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5005 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1060 6GB (0000:04:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import he_normal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plan:\n",
    "\n",
    "1. create decent size sample to work through ideas quickly\n",
    "2. once a good approach is found automate it for full dataset\n",
    "2. __extra! create an ensemble with the sample data frist__\n",
    "3. submit to kaggle 3x times tonight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frozen_vggbn():\n",
    "    vggbn = Vgg16BN()\n",
    "    vggbn = vggbn.model\n",
    "    for layer in vggbn.layers:\n",
    "        layer.trainable = False\n",
    "    return vggbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def half_frozen_vggbn(depth):\n",
    "    '''Transfer learning from vggbn to a frozen model based on given depth.\n",
    "    '''\n",
    "    vggbn = Vgg16BN()\n",
    "    vggbn = vggbn.model\n",
    "    vggbn.pop()\n",
    "    for layer in vggbn.layers[:depth]:\n",
    "        #print('Freezing {}'.format(layer.name))\n",
    "        layer.trainable = False\n",
    "    for layer in vggbn.layers[depth:]:\n",
    "        if 'dense' in layer.name:\n",
    "            #print('Changing {} to he_normal initilizer'.format(layer.name))\n",
    "            layer.kernel_initializer = he_normal()\n",
    "    return vggbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Save results get data from appropriate places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_train_path = '../../dogscats/lrg_sample/train/'\n",
    "sample_val_path = '../../dogscats/lrg_sample/valid/'\n",
    "\n",
    "train_path = '../../dogscats/train/'\n",
    "val_path = '../../dogscats/valid/'\n",
    "\n",
    "sample_results_path = '../../dogscats/lrg_sample/results/'\n",
    "sample_model_path = '../../dogscats/lrg_sample/models/'\n",
    "\n",
    "model_path = '../../dogscats/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Set up data batches. Used both for prototyping but found that augmented is always results in better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_gen = image.ImageDataGenerator(\n",
    "    channel_shift_range=10,\n",
    "    zoom_range=0.05,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=32 #32 - kept running out of mem on proto6\n",
    "\n",
    "train_batches = get_batches(train_path,batch_size=batch_size)\n",
    "aug_train_batches = aug_gen.flow_from_directory(directory=train_path,batch_size=batch_size,shuffle=True,target_size=(224,224))\n",
    "val_batches = get_batches(val_path,batch_size=batch_size*2)\n",
    "\n",
    "train_steps = train_batches.samples//train_batches.batch_size\n",
    "aug_train_steps = aug_train_batches.samples//aug_train_batches.batch_size\n",
    "val_steps = val_batches.samples//val_batches.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ensemble time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proto8():\n",
    "    vggbn = half_frozen_vggbn(-14)\n",
    "    for i in range(5):\n",
    "        vggbn.pop()\n",
    "    vggbn.add(BatchNormalization())\n",
    "    vggbn.add(Dropout(.5))\n",
    "    vggbn.add(Dense(2,activation='softmax',kernel_initializer='he_normal'))\n",
    "    vggbn.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return vggbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proto4():\n",
    "    vggbn = frozen_vggbn()\n",
    "    vggbn.pop()\n",
    "    vggbn.add(Dense(2,activation='softmax',kernel_initializer='he_normal'))\n",
    "    vggbn.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return vggbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'prod8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_func,i,epochs=10):\n",
    "    model = model_func()\n",
    "    cb = [ModelCheckpoint(model_path+'ens_{}_{}'.format(model_name,i), monitor='val_loss', save_best_only=True, save_weights_only=False)]\n",
    "    model.fit_generator(aug_train_batches, aug_train_steps, epochs=epochs, callbacks=cb,\n",
    "                    validation_data=val_batches, validation_steps=val_steps)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Manually build models because kernel will fail if training more than 1 at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "718/718 [==============================] - 345s - loss: 0.1687 - acc: 0.9376 - val_loss: 0.1704 - val_acc: 0.9622\n",
      "Epoch 2/10\n",
      "718/718 [==============================] - 333s - loss: 0.0977 - acc: 0.9651 - val_loss: 1.1543 - val_acc: 0.8202\n",
      "Epoch 3/10\n",
      "718/718 [==============================] - 341s - loss: 0.0941 - acc: 0.9684 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "718/718 [==============================] - 333s - loss: 0.0736 - acc: 0.9747 - val_loss: 0.0807 - val_acc: 0.9747\n",
      "Epoch 5/10\n",
      "718/718 [==============================] - 340s - loss: 0.0673 - acc: 0.9778 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "718/718 [==============================] - 333s - loss: 0.0528 - acc: 0.9815 - val_loss: 0.0735 - val_acc: 0.9804\n",
      "Epoch 7/10\n",
      "718/718 [==============================] - 333s - loss: 0.0555 - acc: 0.9820 - val_loss: 0.0640 - val_acc: 0.9824\n",
      "Epoch 8/10\n",
      "718/718 [==============================] - 340s - loss: 0.0562 - acc: 0.9822 - val_loss: 0.0365 - val_acc: 0.9897\n",
      "Epoch 9/10\n",
      "718/718 [==============================] - 333s - loss: 0.0588 - acc: 0.9810 - val_loss: 0.0424 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "718/718 [==============================] - 333s - loss: 0.0429 - acc: 0.9861 - val_loss: 0.0454 - val_acc: 0.9876\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# model_num = 1\n",
    "# model = train_model(get_proto8,model_num)\n",
    "\n",
    "# model.load_weights(model_path+'ens_{}_{}'.format(model_name,model_num))\n",
    "\n",
    "# test_batches = get_batches('../../dogscats/test/',batch_size=batch_size,shuffle=False)\n",
    "# test_steps = test_batches.n//test_batches.batch_size+1\n",
    "# y_pred = model.predict_generator(test_batches,steps=test_steps)\n",
    "\n",
    "# y_pred = pd.DataFrame(y_pred)\n",
    "# y_pred.to_csv(model_path+'ens_{}_{}_pred'.format(model_name,model_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "718/718 [==============================] - 348s - loss: 0.1685 - acc: 0.9391 - val_loss: 0.3730 - val_acc: 0.8891\n",
      "Epoch 2/10\n",
      "718/718 [==============================] - 345s - loss: 0.1161 - acc: 0.9565 - val_loss: 0.0879 - val_acc: 0.9700\n",
      "Epoch 3/10\n",
      "718/718 [==============================] - 341s - loss: 0.1059 - acc: 0.9632 - val_loss: 0.0752 - val_acc: 0.9799\n",
      "Epoch 4/10\n",
      "718/718 [==============================] - 340s - loss: 0.0742 - acc: 0.9735 - val_loss: 0.0480 - val_acc: 0.9824\n",
      "Epoch 5/10\n",
      "718/718 [==============================] - 333s - loss: 0.0675 - acc: 0.9760 - val_loss: 0.1168 - val_acc: 0.9762\n",
      "Epoch 6/10\n",
      "718/718 [==============================] - 341s - loss: 0.0642 - acc: 0.9778 - val_loss: 0.0358 - val_acc: 0.9876\n",
      "Epoch 7/10\n",
      "718/718 [==============================] - 333s - loss: 0.0538 - acc: 0.9820 - val_loss: 0.0568 - val_acc: 0.9819\n",
      "Epoch 8/10\n",
      "718/718 [==============================] - 333s - loss: 0.0491 - acc: 0.9832 - val_loss: 0.1334 - val_acc: 0.9571\n",
      "Epoch 9/10\n",
      "718/718 [==============================] - 333s - loss: 0.0461 - acc: 0.9841 - val_loss: 0.0460 - val_acc: 0.9871\n",
      "Epoch 10/10\n",
      "718/718 [==============================] - 333s - loss: 0.0392 - acc: 0.9866 - val_loss: 0.0375 - val_acc: 0.9876\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# model_num = 2\n",
    "# model = train_model(get_proto8,model_num)\n",
    "\n",
    "# model.load_weights(model_path+'ens_{}_{}'.format(model_name,model_num))\n",
    "\n",
    "# test_batches = get_batches('../../dogscats/test/',batch_size=batch_size,shuffle=False)\n",
    "# test_steps = test_batches.n//test_batches.batch_size+1\n",
    "# y_pred = model.predict_generator(test_batches,steps=test_steps)\n",
    "\n",
    "# y_pred = pd.DataFrame(y_pred)\n",
    "# y_pred.to_csv(model_path+'ens_{}_{}_pred'.format(model_name,model_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "718/718 [==============================] - 361s - loss: 0.1775 - acc: 0.9314 - val_loss: 0.2169 - val_acc: 0.9511\n",
      "Epoch 2/10\n",
      "718/718 [==============================] - 340s - loss: 0.1073 - acc: 0.9613 - val_loss: 0.1408 - val_acc: 0.9587\n",
      "Epoch 3/10\n",
      "718/718 [==============================] - 338s - loss: 0.0933 - acc: 0.9681 - val_loss: 0.0446 - val_acc: 0.9804\n",
      "Epoch 4/10\n",
      "718/718 [==============================] - 331s - loss: 0.0808 - acc: 0.9731 - val_loss: 0.0782 - val_acc: 0.9690\n",
      "Epoch 5/10\n",
      "718/718 [==============================] - 332s - loss: 0.0730 - acc: 0.9756 - val_loss: 0.1555 - val_acc: 0.9628\n",
      "Epoch 6/10\n",
      "718/718 [==============================] - 332s - loss: 0.0645 - acc: 0.9785 - val_loss: 0.0635 - val_acc: 0.9824\n",
      "Epoch 7/10\n",
      "718/718 [==============================] - 339s - loss: 0.0597 - acc: 0.9804 - val_loss: 0.0435 - val_acc: 0.9840\n",
      "Epoch 8/10\n",
      "718/718 [==============================] - 339s - loss: 0.0496 - acc: 0.9838 - val_loss: 0.0431 - val_acc: 0.9850\n",
      "Epoch 9/10\n",
      "718/718 [==============================] - 331s - loss: 0.0387 - acc: 0.9871 - val_loss: 0.0489 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "718/718 [==============================] - 331s - loss: 0.0450 - acc: 0.9858 - val_loss: 0.0593 - val_acc: 0.9840\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "model_num = 3\n",
    "model = train_model(get_proto8,model_num)\n",
    "\n",
    "model.load_weights(model_path+'ens_{}_{}'.format(model_name,model_num))\n",
    "\n",
    "test_batches = get_batches('../../dogscats/test/',batch_size=batch_size,shuffle=False)\n",
    "test_steps = test_batches.n//test_batches.batch_size+1\n",
    "y_pred = model.predict_generator(test_batches,steps=test_steps)\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred.to_csv(model_path+'ens_{}_{}_pred'.format(model_name,model_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load saved predictions of models and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ens_preds = pd.DataFrame()\n",
    "for i in range(model_num):\n",
    "    df = pd.read_csv(model_path+'lone_{}_{}_pred'.format(model_name,i+1),index_col=0)\n",
    "    ens_preds = pd.concat([ens_preds,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog_preds = ens_preds.drop('0',axis=1)\n",
    "avg_preds = dog_preds.mean(axis=1)\n",
    "avg_preds = np.array(avg_preds)\n",
    "avg_preds = avg_preds.clip(min=0.05, max=0.95)\n",
    "#avg_preds = avg_preds.clip(lower=0.05,upper=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = test_batches.filenames\n",
    "full_files = [f.split('/')[1] for f in filenames]\n",
    "ids = [int(f.split('.')[0]) for f in full_files]\n",
    "formatted = np.stack([ids,avg_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_path = '../../dogscats/submissions/'\n",
    "np.savetxt(sub_path+'lone_{}_full_pred.csv'.format(model_name), formatted, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Submission for the prototype sample ensemble __(simplest ensemble possible)__ of 3 finetuned models trained for 1 epoch each resulted in a score of .12318! thats only .02 worse than the full data 30+ epoch set I submitted yesterday! \n",
    "\n",
    "    LB socre of .12318\n",
    "\n",
    "Now question is which prototype would be best to create an ensemble out of or would more than 1 model in the ensemble be good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "After examining the prototype scores and reading about finetuning multiple layers adding ability to models, I ended up using proto8. Ensembling 3 of these at 10 epochs of training each I got:\n",
    "\n",
    "    LB score of .08918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and submit:\n",
    "\n",
    "---\n",
    "Use this command to download file from server. Must be done from the client *not* from this notebook\n",
    "\n",
    "    scp 96.237.225.57:/home/xbno/anaconda3/courses/dogscats/submissions/submission1.csv ~/Desktop/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
